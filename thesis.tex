\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{setspace}
\usepackage{graphicx}
\usepackage{titling}
\usepackage{textcomp}
\usepackage[nottoc]{tocbibind}
\usepackage{tocloft}
\usepackage{natbib}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{subcaption}
\renewcommand{\cftsecleader}{\cftdotfill{\cftdotsep}}

\usepackage[margin=1.5in]{geometry}
\addtolength{\topmargin}{-0.5in}
\addtolength{\textheight}{1.0in}

\doublespacing
\newenvironment{bottompar}{\par\vspace*{\fill}}{}

\renewcommand{\contentsname}{\center{Table of Contents}}

\title{Real-time Video Alignment and Fusion Using Feature Detection on FPGA Devices}

\author{Robert Haywood Taglang}

\begin{document}
\pagenumbering{gobble}
\begin{titlingpage}
    \vspace*{\fill}
    \begin{center}
        \textbf{\thetitle}
        
        \hfill \\
        \hfill \\
        
        A Thesis \\
        Submitted to the Faculty \\
        of \\
        Drexel University \\
        by \\
        \theauthor \\
        in partial fulfillment of the \\
        requirements for the degree \\
        of \\
        Master of Science in Computer Engineering \\
        June 2017 \\
    \end{center}
    
    \vspace*{\fill}
    
    \begin{figure*}[!b]
    	\centering
       	\includegraphics[width=0.35\textwidth]{drexel-vert-blue}
    \end{figure*}
\end{titlingpage}

\begin{center}
	\vspace*{\fill}
	
	\textcopyright\  Copyright 2017
	
	Robert H. Taglang. All Rights Reserved.
	
	\vspace*{\fill}
\end{center}
\clearpage

\pagenumbering{roman}
\setcounter{page}{2}
\begin{center}
    \tableofcontents
    \clearpage
    
    \listoftables
    \clearpage
    
    \listoffigures
\end{center}
\clearpage

\hfill \\
\hfill \\
\hfill \\
\hfill \\

\addcontentsline{toc}{section}{Abstract}

\begin{abstract}
    \singlespacing
    \centering
    \thetitle \\
    \theauthor \\
    Prawat Nagvajara, Ph.D. \\
    
    \hfill \\
    \hfill \\
    \hfill \\
    
    \doublespacing
    Video fusion functions as a way to combine the important or useful parts of two or more sequences of images. The scenario presented is the use of Laplacian fusion to produce a single video composed of the fields of view of two cameras whose areas of focus differ substantially. This is not a useful real-time strategy unless the frames can be aligned. This thesis presents a system for detecting features using an FPGA implementation of SURF (Speeded-Up Robust Features), and aligning video streams by applying a transform generated from the key features. 
\end{abstract}

\clearpage

\pagenumbering{arabic}

\section{Background}

\subsection{Introduction}

The fusion of data from two or more sensors has been well-researched \cite{wang_multi-focus_2011} \cite{li_multi-sensor_1994}, though these approaches typically discuss the process of fusing images which have been pre-aligned. Pre-computed transforms used to align the frames of two cameras are not robust to variations. Some approaches have made use of additional hardware sensors in order to correct against these variations \cite{chappell_exploiting_2006}. The approach presented in this thesis seeks to perform this correction completely in hardware using feature detection on a FPGA.

The design choice to use a FPGA rather than a GPU or some other software based approach was made due to the advantages gained from operating with embedded hardware, namely higher portability and lower overall power consumption.

\subsection{Laplacian Fusion}

Laplacian pyramids of images have their origin as a strategy for image encoding \cite{burt_laplacian_1983}. A gaussian blur is applied to the image, and the image is downsampled to half of its original size. This process can be repeated on the resulting image to create a sequence of images representing the original in different scale spaces. This sequence of blurred and downsampled images is known as a Gaussian pyramid. An illustration of a Guassian pyramid for an example image can be seen in Figure \ref{fig_pepper_gaussian_pyramid}.

\begin{figure}[h]
	\centering
	\includegraphics[width=0.6\textwidth]{figures/peppers/gaussian_pyramid}
	\caption{Gaussian pyramid of an example image}
	\label{fig_pepper_gaussian_pyramid}
\end{figure}

The Laplacian pyramid is one which can be used for reconstruction of the original image. At each level above the lowest level of the Gaussian pyramid, the level below is upsampled to match the scale of the current level. The difference between the upsampled image and the current scale level image is known as the Laplacian of the image. The sum of the upsampled lower level and the Laplacian is the original image. At a single level, the Laplacian can be thought of as the error introduced by applying a Gaussian and Box filter. A diagram illustrating a Laplacian pyramid can be seen in Figure \ref{fig_pepper_laplacian_pyramid}

\begin{figure}[h]
	\centering
	\includegraphics[width=\textwidth]{figures/peppers/laplacian_pyramid}
	\caption{Laplacian pyramid of an example image}
	\label{fig_pepper_laplacian_pyramid}
\end{figure}

The property of the Laplacian that makes it ideal for fusion is its ability to capture the high frequency components in an image through the use of very simple kernel operators that are easily implemented in hardware. The difference between a blurred image and the original will have higher magnitude in the areas where the image was sharpest.

The fusion of two images can be thought of as a function of the two images $X$ and $Y$ of dimension $M \times N$ where $Z = f(X, Y)$, a single image of dimension $M \times N$. A naive approach to fusion would be to compute the Laplacians and use their magnitudes to select a pixel from either $X$ or $Y$ as shown in Equation \ref{eq_naive_fusion}.

\begin{equation}
Z(i,j)=
\begin{cases}
X(i,j) & |L(X(i,j))| \ge |L(Y(i,j))| \\
Y(i,j) & otherwise
\end{cases}
\label{eq_naive_fusion}
\end{equation}

This approach does not account for variations in colorspace between the two images. Consider the images in Figure \ref{fig_peppers_lr}. The more saturated image will likely have a higher valued Laplacian in some parts simply because it is brighter, therefore having higher magnitudes at individual pixels. This approach also will not facilitate smooth stitching of the images. Contiguous regions of selection from one image will be adjacent to regions from the other with no transition, producing a grainy effect at areas of high frequency. The result of this naive fusion can be seen in Figure \ref{fig_naive_fusion} which exhibits the flaws of this approach.

\begin{figure}[h]
	\centering
	\begin{subfigure}[b]{0.45\textwidth}
		\centering
		\includegraphics[width=\textwidth]{figures/peppers/peppers_blur_left}
		\caption{Blurred on the left; more saturated}
	\end{subfigure}
	\begin{subfigure}[b]{0.45\textwidth}
		\centering
		\includegraphics[width=\textwidth]{figures/peppers/peppers_blur_right}
		\caption{Blurred on the right; more neutral}
	\end{subfigure}
	\caption{Two images of the same scene with variations in sharpness and colorspace}
	\label{fig_peppers_lr}
\end{figure}

\begin{figure}[h]
	\centering
	\begin{subfigure}[b]{0.45\textwidth}
		\centering
		\includegraphics[width=\textwidth]{figures/peppers/peppers_fused_naive}
		\caption{Fusion using the naive approach}
		\label{fig_naive_fusion}
	\end{subfigure}
	\begin{subfigure}[b]{0.45\textwidth}
		\centering
		\includegraphics[width=\textwidth]{figures/peppers/peppers_fused}
		\caption{Fusion using weighted sum}
		\label{fig_weighted_fusion}
	\end{subfigure}
	\caption{Fusion of the images in Figure \ref{fig_peppers_lr} using a naive selection and weighted sum approach}
\end{figure}

A more correct approach would involve using the Laplacian in a weighted sum to combine the pixels of the images, rather than simply selecting them, as shown in Equation \ref{eq_weighted_fusion}.

\begin{equation}
Z(i,j) = \frac{|L(X(i,j))|}{|L(X(i,j))| + |L(Y(i,j))|} \cdot X(i,j) + \frac{|L(Y(i,j))|}{|L(X(i,j))| + |L(Y(i,j))|} \cdot Y(i,j)
\label{eq_weighted_fusion}
\end{equation}

The result of this weighted sum approach can be seen in Figure \ref{fig_weighted_fusion} which exhibits a reduction in graininess from the naive approach.

\subsection{Speeded-up Robust Features (SURF)}

The generation of features for use as marker points in alignment utilizes the SURF algorithm from Bay et al \cite{bay_surf:_2006}. SURF is composed of two parts: a discrete approximation for computing Hessian determinants, and the generation of rotation invariant feature descriptors for detected feature points. 

SURF is typically used for its applications in object recognition, where the feature descriptor is used to facilitate a match between what is observed and some known set of feature points and descriptors. The descriptor largely serves as a way of discriminating against false positives. In terms of using SURF for fusion, the detected feature points will be matched across two images with the assumption that the subject is the same and that the images do contain spatially coherent matches. Given this assumption, it can be concluded that the feature descriptor is not necessary for alignment, only the feature points computed using Hessian determinants.

\subsubsection{Computation of Hessian Determinants}

The Hessian determinant is the determinant of a Hessian matrix, which is a matrix composed of the spatial partial second derivatives of a function $f : \mathbb{R}^n \rightarrow \mathbb{R}$. It is of the general form shown in Equation \ref{eq_hessian_general}. In the image domain, $f : \mathbb{R}^2 \rightarrow \mathbb{R}$. The particular form of the Hessian matrix in $\mathbb{R}^2$ with dimensions $x_1$ and $x_2$ is shown in Equation \ref{eq_hessian_r2}.

\begin{equation}
H = \begin{bmatrix}
\frac{\partial^2 f}{\partial x_1^2} & \frac{\partial^2 f}{\partial x_1 x_2} & \cdots & \frac{\partial^2 f}{\partial x_1 \partial x_n} \\
\frac{\partial^2 f}{\partial x_2 \partial x_1} & \frac{\partial^2 f}{\partial x_2^2} & \cdots & \frac{\partial^2 f}{\partial x_2 \partial x_2} \\
\vdots & \vdots & \ddots & \vdots \\
\frac{\partial^2 f}{\partial x_n \partial x_1} & \frac{\partial^2 f}{\partial x_n \partial x_2} & \cdots & \frac{\partial^2 f}{\partial x_n^2}
\end{bmatrix}
\label{eq_hessian_general}
\end{equation}

\begin{equation}
H = \begin{bmatrix}
\frac{\partial^2 f}{\partial x_1^2} & \frac{\partial^2 f}{\partial x_1 x_2} \\
\frac{\partial^2 f}{\partial x_2 x_1} & \frac{\partial^2 f}{\partial x_2^2}
\end{bmatrix}
\label{eq_hessian_r2}
\end{equation}

The second order derivative used can be computed via convolution of the Gaussian second order derivative at any point, $x$, in the image. The formulas for the Gaussian second order derivative for each partial with respect to $x_1^2$, $x_1 x_2$ and $x_2^2$ can be seen in Equations \ref{eq_partial_x1}, \ref{eq_partial_x1x2}, and \ref{eq_partial_x2} respectively.

\begin{equation}
\frac{\partial^2 G(x_1,x_2,\sigma)}{\partial^2 x_1} = (-1 + \frac{x_1^2}{\sigma^2})\frac{e^{-\frac{x_1^2+x_2^2}{2\sigma^2}}}{2\pi \sigma^4}
\label{eq_partial_x1}
\end{equation}

\begin{equation}
\frac{\partial^2 G(x_1,x_2,\sigma)}{\partial x_1 x_2} = \frac{x_1 x_2}{2 \pi \sigma^6}e^{-\frac{x_1^2 + x_2^2}{2 \sigma^2}}
\label{eq_partial_x1x2}
\end{equation}

\begin{equation}
\frac{\partial^2 G(x_1,x_2,\sigma)}{\partial^2 x_2} = (-1 + \frac{x_2^2}{\sigma^2})\frac{e^{-\frac{x_1^2+x_2^2}{2\sigma^2}}}{2\pi \sigma^4}
\label{eq_partial_x2}
\end{equation}

3D surface plots of these equations where $\sigma=1$ can be seen in Figure \ref{fig_gaussian_surface_plots}. In order to compute these functions quickly, SURF approximates them with cropped, discrete, kernels. 

\begin{figure}[h]
	\centering
	\begin{subfigure}[b]{0.45\textwidth}
		\centering
		\includegraphics[width=\textwidth]{figures/hessian/gaussian_xx}
		\caption{$\frac{\partial^2 G(x_1, x_2)}{\partial^2 x_1}$}
	\end{subfigure}
	\begin{subfigure}[b]{0.45\textwidth}
		\centering
		\includegraphics[width=\textwidth]{figures/hessian/gaussian_xy}
		\caption{$\frac{\partial^2 G(x_1, x_2)}{\partial x_1 x_2}$}
	\end{subfigure}
	\begin{subfigure}[b]{0.45\textwidth}
		\centering
		\includegraphics[width=\textwidth]{figures/hessian/gaussian_yy}
		\caption{$\frac{\partial^2 G(x_1, x_2)}{\partial^2 x_2}$}
	\end{subfigure}
	\caption{3D surface plots of the Gaussian second order derivative functions where $\sigma=1$}
	\label{fig_gaussian_surface_plots}
\end{figure}

The Gaussian second order derivatives can be approximated as $9 \times 9$ kernels with $\sigma=1.2$. These kernels can be seen in Figure \ref{fig_gaussian_kernels}. By adjusting $\sigma$, Hessian determinants can be computed in different scale spaces. This is a concept that SURF draws from the Scale-Invariant Feature Transform (SIFT) from Lowe et al \cite{lowe_distinctive_2004}.

 By detecting features in different scale spaces, SIFT and by proxy, SURF are robust to changes in scale. SIFT accomplished this by downsampling the image to detect at lower order scale spaces. SURF improved on this approach for speed by instead scaling the kernel.

\begin{figure}[h]
	\centering
	\begin{subfigure}[b]{0.3\textwidth}
		\centering
		\includegraphics[width=\textwidth]{figures/hessian/gaussian_second_order_kernel_xx}
		\caption{$\frac{\partial^2 G}{\partial^2 x_1}$}
	\end{subfigure}
	\begin{subfigure}[b]{0.3\textwidth}
		\centering
		\includegraphics[width=\textwidth]{figures/hessian/gaussian_second_order_kernel_xy}
		\caption{$\frac{\partial^2 G}{\partial x_1 x_2}$}
	\end{subfigure}
	\begin{subfigure}[b]{0.3\textwidth}
		\centering
		\includegraphics[width=\textwidth]{figures/hessian/gaussian_second_order_kernel_yy}
		\caption{$\frac{\partial^2 G}{\partial^2 x_2}$}
	\end{subfigure}
	\caption{$9 \times 9$ discrete approximations of the Gaussian second order derivative with $\sigma=1.2$}
	\label{fig_gaussian_kernels}
\end{figure}

Another speed optimization presented in SURF takes advantage of the form of the discrete kernels. Since the approximated kernels are composed of rectangles of constant value, they can be decomposed into a set of box filters. Box filters can be computed quickly with the use of integral images. The general form of the integral image of an $M \times N$ image $I$ is shown in Equation \ref{eq_integral_image}.

\begin{equation}
\int I = \begin{bmatrix}
I(0,0) & \sum\limits_{m=0}^1 I(0,m) & \cdots & \sum\limits_{m=0}^M I(0,m) \\
\sum\limits_{n=0}^1 I(n,0) & \sum\limits_{n=0}^1 \sum\limits_{m=0}^1 I(n,m) & & \vdots \\
\vdots & & \ddots & \vdots \\
\sum\limits_{n=0}^N I(n,0) & \cdots & \cdots & \sum\limits_{n=0}^N \sum\limits_{m=0}^M I(n,m)
\end{bmatrix}
\label{eq_integral_image}
\end{equation}

Integral images decrease the computational complexity of finding the sum of an area in the input image. The computational complexity of strict kernel convolution at each point scales with the size of the kernel and is of the order $O(N^2)$ where the kernel is $N \times N$. A box filter can be decomposed into finding the sum of an area in the image and scaling it. Finding the sum from the integral image can be performed in $O(1)$ as shown in Equation \ref{eq_integral_image_box}. 

\begin{equation}
\sum\limits_{n=w}^y \sum\limits_{m=x}^z = \int I(y,z) - \int I(w-1,z) - \int I(y,x-1) + \int I(w-1, x-1)
\label{eq_integral_image_box}
\end{equation}

This makes the computation of the Hessian matrix scale only in terms of the image size, yielding no additional penalty for operating on different scale spaces.

\subsubsection{SURF Implementations for FPGA Devices}

The relatively low computational complexity makes SURF a popular choice for FPGA applications. Battezzati et al. present an architecture using accumulators for computing the integral image pipelined through the Hessian computation and storing detected feature points in a FIFO cache \cite{battezzati_surf_2012}. These are matched against a stored set of feature points. Chen et al. present improvements on this approach by parallelizing the computation of different scale spaces \cite{chen_fpga-based_2016}. The implementation in this thesis follows these approaches with some additional improvements for speed based on the use case of matching against another image rather than a stored set of features.

\subsection{Iterative Closest Point Algorithm}

The crucial step in aligning the two images is the computation of an affine transform mapping one image into the space of the other. Once the images are aligned, they can be fused. A combination of SURF and the iterative closest point algorithm are used to compute this transform. Iterative closest point was first proposed by Chen and Medioni as a method for aligning 3-D point cloud data \cite{chen_object_1992}. In its simplest form, the algorithm follows the following steps:

\begin{enumerate}
	\item Each point in the set of points to be transformed is matched against the closest (usually Euclidean distance) point in the reference set of points.
	\item A transformation is estimated to minimize the distance between the transform set and their matches in the reference set of points.
	\item The transform is applied to the points.
\end{enumerate}

This process is repeated, converging to the local minimum that is the match between the two point sets.

Let $X_p(i)$ be be the $i^{th}$ point in the set of reference points onto which the transform points, $X(j)$, where $j$ is the index of the closest point to $X_p(i)$ in $X$, will be projected. The general form for this transformation $M$ is shown in Equation \ref{eq_general_icp_mapping}, and the expanded matrix form can be seen in Equation \ref{eq_expanded_icp_mapping}. In the expanded matrix form $M$ is decomposed into $R$, a $2 \times 2$ rotation matrix, and $T$, a translation offset.

\begin{equation}
X_p(i) = X(j) \cdot M
\label{eq_general_icp_mapping}
\end{equation}

\begin{equation}
\begin{bmatrix}
X_p(i)_1 \\
X_p(i)_2 \\
1 
\end{bmatrix}
=
\begin{bmatrix}
X(j)_1 & X(j)_2 & 1
\end{bmatrix}
\cdot
\begin{bmatrix}
R_{11} & R_{12} & T_1 \\
R_{21} & R_{22} & T_2 \\
0 & 0 & 1
\end{bmatrix}
\label{eq_expanded_icp_mapping}
\end{equation}

This equation only solves for $M$ for a single point relation, but can be restructured to contain
the whole set of $N$ points as shown in Equation \ref{eq_full_icp_matrix}. In this equation $P$ is a function $P: i \rightarrow j$ mapping the closest points in each set.

\begin{equation}
\begin{bmatrix}
X_p(0)_1 \\
X_p(0)_2 \\
X_p(1)_1 \\
X_p(1)_2 \\
\vdots \\
X_p(N)_1 \\
X_p(N)_2
\end{bmatrix}
=
\begin{bmatrix}
X(P(0))_1 & X(P(0))_2 & 1 & 0 & 0 & 0 \\
0 & 0 & 0 & X(P(0))_1 & X(P(0))_2 & 1 \\
X(P(1))_1 & X(P(1))_2 & 1 & 0 & 0 & 0 \\
0 & 0 & 0 & X(P(1))_1 & X(P(1))_2 & 1 \\
\vdots & & & & & \vdots \\
X(P(N))_1 & X(P(N))_2 & 1 & 0 & 0 & 0 \\
0 & 0 & 0 & X(P(N))_1 & X(P(N))_2 & 1 
\end{bmatrix}
\cdot
\begin{bmatrix}
R_{11} \\
R_{12} \\
T_1 \\
R_{21} \\
R_{22} \\
T_2
\end{bmatrix}
\label{eq_full_icp_matrix}
\end{equation}

Solving for $M$ in this way maps all points in $X$ to their closest points in $X_p$ with the error minimized in a least-squares sense. 

In this form, the transform $M$ will include shear transformations and non-uniform scaling. The computation can be simplified by forcing the second basis vector in $R$ to be orthogonal to the first. The two cameras are expected to be be physically in the same plane, and as such, non-uniform scaling and shear transformations are not expected for alignment. By setting $R_{21} = -R_{12}$ and $R_{22} = R{11}$, the computation of $M$ can be reduced as shown in Equation \ref{eq_reduced_icp_matrix}.

\begin{equation}
\begin{bmatrix}
X_p(0)_1 \\
X_p(0)_2 \\
X_p(1)_1 \\
X_p(1)_2 \\
\vdots \\
X_p(N)_1 \\
X_p(N)_2
\end{bmatrix}
=
\begin{bmatrix}
X(P(0))_1 & X(P(0))_2 & 1 & 0 \\
X(P(0))_2 & -X(P(0))_1 & 0 & 1 \\
X(P(1))_1 & X(P(1))_2 & 1 & 0 \\
X(P(1))_2 & -X(P(1))_1 & 0 & 1 \\
\vdots & & & \vdots \\
X(P(N))_1 & X(P(N))_2 & 1 & 0 \\
X(P(N))_2 & -X(P(N))_1 & 0 & 1 \\
\end{bmatrix}
\cdot
\begin{bmatrix}
R_{11} \\
R_{12} \\
T_1 \\
T_2
\end{bmatrix}
\label{eq_reduced_icp_matrix}
\end{equation}

The performance of iterative closest point can be further improved by making it more sensitive to errors. Chetverikov introduces a variant of iterative closest point referred to as trimmed iterative closest point (TrICP) \cite{chetverikov_trimmed_2002}. TrICP is more robust to errors by eliminating points that introduce error into the matching. Some detected features points will not have correspondences between images. By eliminating these points, the overall error can be reduced to compute a more accurate transform.

\subsection{Singular Value Decomposition (SVD)}

Let Equation \ref{eq_reduced_icp_matrix} be of the form $X=Q \cdot M$. In order to compute compute the transform $M$, the equation must be restructured as in Equation \ref{eq_svd_inverse}.

\begin{equation}
Q^{-1} X = M
\label{eq_svd_inverse}
\end{equation}

$Q$ is not a square matrix, and as such is not invertible, but its pseudoinverse can be used in this instance. The pseudoinverse is computed through the use of singular value decomposition. The use of singular value decomposition to compute the transform is the source of the least squares fitting achieved by iterative closest point.

\subsubsection{SVD Implementations for FPGA Devices}

Ledesma-Carillo et al. present a hardware efficient algorithm for computing singular value decompositions on large matrices uisng one-sided Jacobi rotations \cite{ledesma-carrillo_reconfigurable_2011}.

\section{Implementation}

\subsection{Streaming Kernel Operators}

\subsubsection{Hessian Kernel with Accumulator}

\subsubsection{Box Kernel Approximation of Single Level Laplacian Pyramid}

\subsection{Computation of Transform from Detected Features}

\subsection{Application of Transform to Real-Time Data}

\section{Results}

\begin{figure}[h]
	\centering
	\includegraphics[width=\textwidth]{figures/alignment/error_for_iterations}
	\caption{The error in pixels per feature for performing $n$ iterations of the iterative closest point algorithm on differently sized sets of the strongest feature points}
	\label{fig_results_error_plot}
\end{figure}

\begin{figure}[h]
	\centering
	\begin{subfigure}[b]{0.3\textwidth}
		\centering
		\includegraphics[width=\textwidth]{figures/alignment/fused_20_features_16_iterations}
		\caption{20 features}
	\end{subfigure}
	\begin{subfigure}[b]{0.3\textwidth}
		\centering
		\includegraphics[width=\textwidth]{figures/alignment/fused_40_features_16_iterations}
		\caption{40 features}
	\end{subfigure}
	\begin{subfigure}[b]{0.3\textwidth}
		\centering
		\includegraphics[width=\textwidth]{figures/alignment/fused_80_features_16_iterations}
		\caption{80 features}
	\end{subfigure}

   	\begin{subfigure}[b]{0.3\textwidth}
    	\centering
    	\includegraphics[width=\textwidth]{figures/alignment/fused_160_features_16_iterations}
    	\caption{160 features}
    \end{subfigure}
    \begin{subfigure}[b]{0.3\textwidth}
    	\centering
    	\includegraphics[width=\textwidth]{figures/alignment/fused_320_features_16_iterations}
    	\caption{320 features}
    \end{subfigure}
    \begin{subfigure}[b]{0.3\textwidth}
    	\centering
    	\includegraphics[width=\textwidth]{figures/alignment/fused_640_features_16_iterations}
    	\caption{640 features}
    \end{subfigure}
	\caption{Alignment results after running iterative closest point for 16 iterations on varying numbers of features with reference features marked in red and transformed features in blue}
	\label{fig_alignment_results}
\end{figure}

\begin{figure}[h]
	\centering
	\includegraphics[width=\textwidth]{figures/alignment/error_for_alignment}
	\caption{The error in intensity based on the alignment from performing $n$ iterations of the iterative closest point algorithm on differently sized sets of the strongest feature points}
	\label{fig_results_alignment_error_plot}
\end{figure}

\pagebreak

\bibliographystyle{plain}
\bibliography{thesis}

\end{document}
